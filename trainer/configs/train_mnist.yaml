project: 'classification-sample'
name: 'test_00'

base_lr: 0.0001
devcies: [0]
max_epochs: 50
save_dir: '/opt/simple-trainer/logs' # path to save logs

config_datamodule:
  _target_: univdt.datamodules.BaseDataModule
  data_dir: '/opt/simple-trainer/data' # path to your data
  datasets: 'mnist' # e.g., mnist
  num_workers: 0
  
  split_train: 'train'
  split_val: 'test'
  split_test: 'test'

  batch_size: 512
  batch_size_train: 512
  batch_size_val: 128
  batch_size_test: 128

  transforms_train:
    random_resize:
      height: 32
      width: 32
    random_flip:
      p: 0.5
    random_blur:
      magnitude: 0.2
      p: 0.5
    random_gamma:
      magnitude: 0.2
      p: 0.5
    random_noise:
      magnitude: 0.2
      p: 0.5
    random_brightness:
      magnitude: 0.2
      p: 0.5
    random_contrast:
      magnitude: 0.2
      p: 0.5

  transforms_val:
    resize:
      height: 32
      width: 32
  
  transforms_test:
    resize:
      height: 32
      width: 32
    


config_model:
  _target_: trainer.models.Model
  num_classes: 10
  encoder:
    name: 'resnet10t.c3_in1k'
    in_chans: 1
    out_indices: [4]
  decoder:
    name: 'identity'
  header:
    name: 'linear'    
    return_logits: True
    

config_optimizer:
  _target_: torch.optim.RAdam
  lr: ${base_lr}

config_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_epochs}
  eta_min: 0.001 # base_lr * 0.01

config_engine:
  _target_: trainer.engine.ClassificationEngine
  criterion: ce
    

config_logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: ${project}
  name: ${name}
  save_dir: ${save_dir}
  offline: False
  id: null 
  log_model: False
  prefix: ""
  job_type: "train"
  group: ""
  tags: []


config_trainer:
  _target_:  pytorch_lightning.Trainer
  min_epochs: 1
  max_epochs: ${max_epochs}
  
  accelerator: 'gpu'
  devices: ${devcies}
  precision: '16-mixed' # auto mixed precsion
  accumulate_grad_batches: 1 # gradient accumulation
  
  log_every_n_steps: 1
  num_sanity_val_steps: 3